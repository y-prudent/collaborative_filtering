{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Yannick PRUDENT</b>\n",
    "\n",
    "<center><h1>Collaborative filtering recommander systems</h1></center>\n",
    "\n",
    "___\n",
    "\n",
    "Sources:\n",
    "<p>\n",
    "<a id=\"doc_01\">[1]</a> \n",
    "Ekstrand, Michael D. « <a href=http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf>Collaborative Filtering Recommender Systems</a> ». Foundations and Trends® in Human–Computer Interaction 4, nᵒ 2 (2011): 81‑173.<br>\n",
    "<a id=\"doc_02\">[2]</a>\n",
    "<a href=https://www.lri.fr/~antoine/Courses/Master-ISI/section-app-collaboratif.pdf>Apprentissage pour le filtrage collaboratif</a>. (n.d).<br>\n",
    "<a id=\"doc_03\">[3]</a>\n",
    "Anand, A. (2020, October 3). <a href=https://towardsdatascience.com/user-user-collaborative-filtering-for-jokes-recommendation-b6b1e4ec8642>User-User Collaborative Filtering For Jokes Recommendation</a> - Towards Data Science.<br>\n",
    "<a id=\"doc_04\">[4]</a>\n",
    "Grover, P. (2020, July 16). <a href=https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0>Various Implementations of Collaborative Filtering</a> - Towards Data Science.<br>\n",
    "<a id=\"doc_05\">[5]</a>\n",
    "<a href=https://surprise.readthedocs.io/en/v1.1.1/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization>Matrix Factorization-based algorithms</a> - scikit-surprise (python library) documentation.<br> \n",
    "<a id=\"doc_06\">[6]</a>\n",
    "Malaeb, M. (2018, June 15). <a href=https://medium.com/@m_n_malaeb/singular-value-decomposition-svd-in-recommender-systems-for-non-math-statistics-programming-4a622de653e9>Singular Value decomposition (SVD) in recommender systems for Non-math-statistics-programming wizards</a> - Medium.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We see the use of **recommendation systems** all around us. These systems are personalizing our web experience, telling us what to buy (Amazon), which movies to watch (Netflix), whom to be friends with (Facebook), which songs to listen (Spotify) etc. These recommendation systems leverage our shopping/ watching/ listening patterns and **predict what we could like in future based on our behavior patterns**. \n",
    "\n",
    "The most basic models for recommendations systems are **Collaborative Filtering (CF) models** which are based on assumption that **people like things similar to other things they like, and things that are liked by other people with similar taste**.\n",
    "\n",
    "**In this notebook**, we are going to see the principle of a few CF methods (**Memory-based approach** / **Model-based approach**). Then we will end with a quick 'Get Started' tutorial of the **scikit-surprise** python library, widely used in CF.\n",
    "\n",
    "<img src='pics/recommander_systems.png' width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Memory based approach\n",
    "\n",
    "**Memory-Based Collaborative Filtering approaches** can be divided into two main sections: **user-user filtering** and **item-item filtering**. \n",
    "\n",
    "- A **user-user filtering** takes a particular user, find users that are similar to that user based on similarity of ratings, and recommend items that those similar users liked. \n",
    "\n",
    "- In contrast, **item-item filtering** will take an item, find users who liked that item, and find other items that those users or similar users also liked. It takes items and outputs other items as recommendations.\n",
    "\n",
    "<img src='pics/memory_based.png' width=500 />\n",
    "\n",
    "\n",
    "## 1.1 User-user filtering [[1]](#doc_01) [[2]](#doc_02) [[3]](#doc_03)\n",
    "\n",
    "We will illustrate the **user-user filtering method** with a homemade implementation of the algorithm, using the `pandas` and `numpy` python libraries.\n",
    "\n",
    "### 1.1.0 - The Jester dataset\n",
    "\n",
    "We will use the **joke** \"Dataset 1\" of the **Jester Research project** (UC Berkeley Laboratory for Automation Science and Engineering). [(link here)](http://eigentaste.berkeley.edu/dataset/)\n",
    "\n",
    "- It consists in ratings for **100 jokes** from **24,983 readers** who have rated 36 or more jokes \n",
    "- The rating values are ranging **from -10.00 to +10.00** (the value \"99\" corresponds to \"null\" = \"not rated\")\n",
    "\n",
    "### 1.1.1 - Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "jokes_dir_path = os.path.abspath('data/jester_dataset')\n",
    "\n",
    "# Import data\n",
    "df = pd.read_excel(os.path.join(jokes_dir_path, 'jester-data-1.xls'), header=None)\n",
    "\n",
    "n_ratings_s = df.iloc[:,0]  # Number of ratings per user\n",
    "rating_df = df.drop(labels=[0], axis=1)  # Rating matrix\n",
    "\n",
    "# Fill '99' values with NaN\n",
    "rating_df = rating_df.replace({99: np.NaN})\n",
    "\n",
    "# Rename the columns\n",
    "rating_df.rename(columns={c:f'joke_{c}' for c in rating_df.columns},\n",
    "                     inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example, we will only use the 10 first jokes for the 10 first users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_1</th>\n",
       "      <th>joke_2</th>\n",
       "      <th>joke_3</th>\n",
       "      <th>joke_4</th>\n",
       "      <th>joke_5</th>\n",
       "      <th>joke_6</th>\n",
       "      <th>joke_7</th>\n",
       "      <th>joke_8</th>\n",
       "      <th>joke_9</th>\n",
       "      <th>joke_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.82</td>\n",
       "      <td>8.79</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-8.16</td>\n",
       "      <td>-7.52</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-9.85</td>\n",
       "      <td>4.17</td>\n",
       "      <td>-8.98</td>\n",
       "      <td>-4.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.08</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>6.36</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>8.88</td>\n",
       "      <td>9.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80</td>\n",
       "      <td>8.16</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>6.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.50</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>-5.39</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>5.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   joke_1  joke_2  joke_3  joke_4  joke_5  joke_6  joke_7  joke_8  joke_9  \\\n",
       "0   -7.82    8.79   -9.66   -8.16   -7.52   -8.50   -9.85    4.17   -8.98   \n",
       "1    4.08   -0.29    6.36    4.37   -2.38   -9.66   -0.73   -5.34    8.88   \n",
       "2     NaN     NaN     NaN     NaN    9.03    9.27    9.03    9.27     NaN   \n",
       "3     NaN    8.35     NaN     NaN    1.80    8.16   -2.82    6.21     NaN   \n",
       "4    8.50    4.61   -4.17   -5.39    1.36    1.60    7.04    4.61   -0.44   \n",
       "\n",
       "   joke_10  \n",
       "0    -4.76  \n",
       "1     9.22  \n",
       "2      NaN  \n",
       "3     1.84  \n",
       "4     5.73  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = 10\n",
    "n_jokes = 10\n",
    "\n",
    "# We keep the 10 first jokes and the 10 first users\n",
    "data = rating_df.iloc[:n_users, :n_jokes]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 - Normalization\n",
    "\n",
    "We will normalize the data by subtracting the mean rating of each user, and then by filling the NaN with zeros.\n",
    "\n",
    "$$\\hat{r}_{u, i} \\leftarrow r_{u, i} - \\bar{r}_u, \\qquad \\forall (u, i) \\in (U, I)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_1</th>\n",
       "      <th>joke_2</th>\n",
       "      <th>joke_3</th>\n",
       "      <th>joke_4</th>\n",
       "      <th>joke_5</th>\n",
       "      <th>joke_6</th>\n",
       "      <th>joke_7</th>\n",
       "      <th>joke_8</th>\n",
       "      <th>joke_9</th>\n",
       "      <th>joke_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.591</td>\n",
       "      <td>14.019000</td>\n",
       "      <td>-4.431</td>\n",
       "      <td>-2.931</td>\n",
       "      <td>-2.291000</td>\n",
       "      <td>-3.271000</td>\n",
       "      <td>-4.621000</td>\n",
       "      <td>9.399000</td>\n",
       "      <td>-3.751</td>\n",
       "      <td>0.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.629</td>\n",
       "      <td>-1.741000</td>\n",
       "      <td>4.909</td>\n",
       "      <td>2.919</td>\n",
       "      <td>-3.831000</td>\n",
       "      <td>-11.111000</td>\n",
       "      <td>-2.181000</td>\n",
       "      <td>-6.791000</td>\n",
       "      <td>7.429</td>\n",
       "      <td>7.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.120000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>-0.120000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>4.426667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.123333</td>\n",
       "      <td>4.236667</td>\n",
       "      <td>-6.743333</td>\n",
       "      <td>2.286667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.155</td>\n",
       "      <td>2.265000</td>\n",
       "      <td>-6.515</td>\n",
       "      <td>-7.735</td>\n",
       "      <td>-0.985000</td>\n",
       "      <td>-0.745000</td>\n",
       "      <td>4.695000</td>\n",
       "      <td>2.265000</td>\n",
       "      <td>-2.785</td>\n",
       "      <td>3.385000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   joke_1     joke_2  joke_3  joke_4    joke_5     joke_6    joke_7    joke_8  \\\n",
       "0  -2.591  14.019000  -4.431  -2.931 -2.291000  -3.271000 -4.621000  9.399000   \n",
       "1   2.629  -1.741000   4.909   2.919 -3.831000 -11.111000 -2.181000 -6.791000   \n",
       "2   0.000   0.000000   0.000   0.000 -0.120000   0.120000 -0.120000  0.120000   \n",
       "3   0.000   4.426667   0.000   0.000 -2.123333   4.236667 -6.743333  2.286667   \n",
       "4   6.155   2.265000  -6.515  -7.735 -0.985000  -0.745000  4.695000  2.265000   \n",
       "\n",
       "   joke_9   joke_10  \n",
       "0  -3.751  0.469000  \n",
       "1   7.429  7.769000  \n",
       "2   0.000  0.000000  \n",
       "3   0.000 -2.083333  \n",
       "4  -2.785  3.385000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(df):\n",
    "    df = df.subtract(df.mean(axis=1), axis='rows')\n",
    "    return df.fillna(0)\n",
    "\n",
    "norm_data = normalize(data)\n",
    "\n",
    "norm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 - Computing similarities\n",
    "\n",
    "We need to compute the similarities between each user. Several different similarity functions have been proposed and evaluated in the literature: \n",
    "\n",
    "#### 1.1.3.1 - Pearson correlation\n",
    "\n",
    "This method computes the statistical correlation (Pearson’s r) between two user’s common ratings to determine their similarity.\n",
    "\n",
    "$$s(u, u') =  \\frac{\\sum_{i \\in I_u \\cap I_{u'}} (r_{u,i}-\\bar{r}_u)(r_{u',i}-\\bar{r}_{u'})}\n",
    "            {\\sqrt{\\sum_{i \\in I_u \\cap I_{u'}} (r_{u,i}-\\bar{r}_u)^2}\n",
    "            \\sqrt{\\sum_{i \\in I_u \\cap I_{u'}} (r_{u',i}-\\bar{r}_{u'})^2}}$$\n",
    "\n",
    "#### 1.1.3.2 - Spearman rank correlation\n",
    "\n",
    "For the Spearman correlation, the items a user has rated are ranked such that their highest-rated item is at rank 1 and lower-rated items have higher ranks. Items with the same rating are assigned the average rank for their position. The computation is then the same as that of the Pearson correlation, except that ranks are used in place of ratings.\n",
    "\n",
    "#### 1.1.3.3 - Cosine similarity\n",
    "\n",
    "$$s(u, u') = \\frac{\\textbf{r}_u \\cdot \\textbf{r}_{u'}}\n",
    "              {\\lVert \\textbf{r}_u \\lVert_2 \\lVert \\textbf{r}_{u'} \\lVert_2}\n",
    "           = \\frac{\\sum_i r_{u,i} r_{u',i}}\n",
    "              {\\sqrt{\\sum_i r^2_{u,i}} \\sqrt{\\sum_i r^2_{u',i}}}$$\n",
    "              \n",
    "Now let's compute these similarity functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# Pearson similarity\n",
    "def similarity_pearson(x, y):\n",
    "    return scipy.stats.pearsonr(x, y)[0]\n",
    "\n",
    "# Spearman similarity\n",
    "def similarity_spearman(x, y):\n",
    "    return scipy.stats.spearmanr(x, y)[0]\n",
    "\n",
    "# Cosine similarity\n",
    "def similarity_cosine(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "\n",
    "similarities = {'pearson': similarity_pearson,\n",
    "                'spearman': similarity_spearman,\n",
    "                'cosine': similarity_cosine}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the similarity matrixes for the different similarity functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_similarity_matrix(similarity, norm_ratings, n_users):\n",
    "    similarity_matrix = np.array([similarity(norm_ratings.iloc[i, :], norm_ratings.iloc[j, :])\n",
    "                             for i in range(n_users) for j in range(n_users)])\n",
    "    return similarity_matrix.reshape(n_users, n_users)\n",
    "\n",
    "\n",
    "# Visualize the similarity matrixes\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, (name, sim_function) in enumerate(similarities.items()):\n",
    "    # Computing similarity matrix\n",
    "    similarity_matrix = compute_similarity_matrix(sim_function, norm_data, n_users)\n",
    "\n",
    "    # Plot the matrix\n",
    "    ax = fig.add_subplot(1, len(similarities), i+1)\n",
    "    cax = ax.matshow(similarity_matrix, cmap='plasma')\n",
    "    ax.set_title(f'{name.capitalize()} similarity matrix')\n",
    "    fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    <b>Note:</b> The cosine similarity on centered vectors is equivalent to pearson's correlation. This is why we get the same similarity matrices for Pearson and Cosine after applying our normalization.\n",
    "</div>\n",
    "\n",
    "For our example, we will use **Pearson's correlation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_name = 'pearson'\n",
    "sim_function = similarities[similarity_name]\n",
    "\n",
    "similarity_matrix = compute_similarity_matrix(similarity_pearson, norm_data, n_users)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 -  Selecting the neighbours\n",
    "\n",
    "We need to select the nearest users from the user we want to recommend some items. There are different approaches.\n",
    "\n",
    "#### 1.1.4.1 - Threshold\n",
    "\n",
    "Among the users which have rated the item $i$, this method **selects the users whose similarity value is over a threshold $s_{\\text{min}}$**.\n",
    "\n",
    "$$N_{u, i} = \\{u' \\quad | \\quad s(u, u') > s_{\\text{min}}, \\quad u' \\in U_i\\}$$ with:\n",
    "\n",
    "- $N_{u,i}$  : Neighbours of the user $u$ which have rated the item $i$\n",
    "- $s(u, u')$  : Similarity between the users $u$ and $u'$\n",
    "- $s_{\\text{min}}$  : Threshold value of similarity\n",
    "- $U_{i}$  : Users which have rated the item $i$\n",
    "\n",
    "#### 1.1.4.2 - K-nearest neighbours\n",
    "\n",
    "Among the users which have rated the item $i$, this method **selects the K-most similar users to the user $u$**, according to the similarity function.\n",
    "\n",
    "$$N_{u, i} = \\{u' \\quad | \\quad s(u, u') \\geq M^K_{u,i}, \\quad u' \\in U_i\\}$$ with:\n",
    "\n",
    "- $N_{u,i}$  : Neighbours of the user $u$ which have rated the item $i$\n",
    "- $s(u, u')$  : Similarity between the users $u$ and $u'$\n",
    "- $M^k_{u,i}$ : The $K^{\\text{th}}$ highest value of $s(u,\\cdot)$ in $U_i$\n",
    "- $U_{i}$  : Users which have rated the item $i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbours_threshold(similarity_df, rating_df, user_id, item, threshold=0.1):\n",
    "    # We select the indexes of the users whose rating is not null for the item\n",
    "    users_notnull = rating_df.index[rating_df[item].notnull()]\n",
    "    \n",
    "    # Similarities between these users and the user of interest\n",
    "    sim = similarity_df.iloc[users_notnull, user_id]\n",
    "    \n",
    "    # Return the users which similarity is over the threshold\n",
    "    return [sim.index[i] for i, v in enumerate(sim) if v>=threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we will predict the ratings on the user 2's unrated items. Let's get his neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col0,#T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col1,#T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col2,#T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col3,#T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col8,#T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col9{\n",
       "            background-color:  orange;\n",
       "        }</style><table id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >joke_1</th>        <th class=\"col_heading level0 col1\" >joke_2</th>        <th class=\"col_heading level0 col2\" >joke_3</th>        <th class=\"col_heading level0 col3\" >joke_4</th>        <th class=\"col_heading level0 col4\" >joke_5</th>        <th class=\"col_heading level0 col5\" >joke_6</th>        <th class=\"col_heading level0 col6\" >joke_7</th>        <th class=\"col_heading level0 col7\" >joke_8</th>        <th class=\"col_heading level0 col8\" >joke_9</th>        <th class=\"col_heading level0 col9\" >joke_10</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row0_col0\" class=\"data row0 col0\" >-7.820000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row0_col1\" class=\"data row0 col1\" >8.790000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row0_col2\" class=\"data row0 col2\" >-9.660000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row0_col3\" class=\"data row0 col3\" >-8.160000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row0_col4\" class=\"data row0 col4\" >-7.520000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row0_col5\" class=\"data row0 col5\" >-8.500000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row0_col6\" class=\"data row0 col6\" >-9.850000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row0_col7\" class=\"data row0 col7\" >4.170000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row0_col8\" class=\"data row0 col8\" >-8.980000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row0_col9\" class=\"data row0 col9\" >-4.760000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row1_col0\" class=\"data row1 col0\" >4.080000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row1_col1\" class=\"data row1 col1\" >-0.290000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row1_col2\" class=\"data row1 col2\" >6.360000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row1_col3\" class=\"data row1 col3\" >4.370000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row1_col4\" class=\"data row1 col4\" >-2.380000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row1_col5\" class=\"data row1 col5\" >-9.660000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row1_col6\" class=\"data row1 col6\" >-0.730000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row1_col7\" class=\"data row1 col7\" >-5.340000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row1_col8\" class=\"data row1 col8\" >8.880000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row1_col9\" class=\"data row1 col9\" >9.220000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col0\" class=\"data row2 col0\" >nan</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col1\" class=\"data row2 col1\" >nan</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col2\" class=\"data row2 col2\" >nan</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col3\" class=\"data row2 col3\" >nan</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col4\" class=\"data row2 col4\" >9.030000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col5\" class=\"data row2 col5\" >9.270000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col6\" class=\"data row2 col6\" >9.030000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col7\" class=\"data row2 col7\" >9.270000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col8\" class=\"data row2 col8\" >nan</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row2_col9\" class=\"data row2 col9\" >nan</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row3_col0\" class=\"data row3 col0\" >nan</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row3_col1\" class=\"data row3 col1\" >8.350000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row3_col2\" class=\"data row3 col2\" >nan</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row3_col3\" class=\"data row3 col3\" >nan</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row3_col4\" class=\"data row3 col4\" >1.800000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row3_col5\" class=\"data row3 col5\" >8.160000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row3_col6\" class=\"data row3 col6\" >-2.820000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row3_col7\" class=\"data row3 col7\" >6.210000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row3_col8\" class=\"data row3 col8\" >nan</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row3_col9\" class=\"data row3 col9\" >1.840000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row4_col0\" class=\"data row4 col0\" >8.500000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row4_col1\" class=\"data row4 col1\" >4.610000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row4_col2\" class=\"data row4 col2\" >-4.170000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row4_col3\" class=\"data row4 col3\" >-5.390000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row4_col4\" class=\"data row4 col4\" >1.360000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row4_col5\" class=\"data row4 col5\" >1.600000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row4_col6\" class=\"data row4 col6\" >7.040000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row4_col7\" class=\"data row4 col7\" >4.610000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row4_col8\" class=\"data row4 col8\" >-0.440000</td>\n",
       "                        <td id=\"T_97dac1d0_5289_11eb_9b31_3af9d37f3df4row4_col9\" class=\"data row4 col9\" >5.730000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff464e68cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's select the neighbours of the user_2 for each of his unpredicted items (nan):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbours_user_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>joke_1</th>\n",
       "      <td>[0, 5, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joke_2</th>\n",
       "      <td>[0, 3, 5, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joke_3</th>\n",
       "      <td>[0, 5, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joke_4</th>\n",
       "      <td>[0, 5, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joke_9</th>\n",
       "      <td>[0, 5, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joke_10</th>\n",
       "      <td>[0, 3, 5, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        neighbours_user_2\n",
       "joke_1          [0, 5, 8]\n",
       "joke_2       [0, 3, 5, 8]\n",
       "joke_3          [0, 5, 8]\n",
       "joke_4          [0, 5, 8]\n",
       "joke_9          [0, 5, 8]\n",
       "joke_10      [0, 3, 5, 8]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# We will do some predictions on the user 2\n",
    "user_id = 2\n",
    "\n",
    "# Highlight the nan values of the user 2 in the rating matrix\n",
    "display(data.loc[:4].style.highlight_null(null_color='orange', subset=pd.IndexSlice[user_id, :]))\n",
    "\n",
    "# We select the unpredicted items for the user 2\n",
    "recommendation_columns = data.iloc[user_id, :]\n",
    "recommendation_columns = recommendation_columns[recommendation_columns.isna()].index\n",
    "\n",
    "neighbours_df = pd.DataFrame(index=recommendation_columns, columns=[f'neighbours_user_{user_id}'])\n",
    "\n",
    "for i, item in enumerate(recommendation_columns):\n",
    "    neighbours_df.loc[item, f'neighbours_user_{user_id}'] = neighbours_threshold(similarity_df, data, user_id, item)\n",
    "\n",
    "print(f'Let\\'s select the neighbours of the user_{user_id} for each of his unpredicted items (nan):')\n",
    "display(neighbours_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 - Prediction of the ratings\n",
    "\n",
    "Now we will predict the ratings of the user 2 for all of his unrated items.\n",
    "\n",
    "$$p_{u,i} = \\bar{r}_u +\n",
    "            \\frac{\\sum_{u' \\in N_{u,i}} s(u,u')(r_{u',i}-\\bar{r}_{u'})}\n",
    "            {\\sum_{u' \\in N_{u,i}} |s(u,u')|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_user_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>joke_1</th>\n",
       "      <td>8.079033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joke_2</th>\n",
       "      <td>14.038126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joke_3</th>\n",
       "      <td>9.497857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joke_4</th>\n",
       "      <td>6.173935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joke_9</th>\n",
       "      <td>6.612407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joke_10</th>\n",
       "      <td>9.742122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred_user_2\n",
       "joke_1      8.079033\n",
       "joke_2     14.038126\n",
       "joke_3      9.497857\n",
       "joke_4      6.173935\n",
       "joke_9      6.612407\n",
       "joke_10     9.742122"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(user_id, neighbours_df, similarity_df, rating_df):\n",
    "    user_mean = np.mean(rating_df.iloc[user_id, :])\n",
    "    score = np.array([])\n",
    "\n",
    "    for item in neighbours_df.index:\n",
    "        nbs = neighbours_df.loc[item, f'neighbours_user_{user_id}']\n",
    "        nbs_similarity = similarity_df.loc[nbs, user_id]\n",
    "        nbs_norm_rating = rating_df.loc[nbs, item] - np.mean(rating_df, axis=1)[nbs]\n",
    "\n",
    "        score = np.append(score, np.dot(nbs_similarity, nbs_norm_rating) / \n",
    "                          np.sum(np.abs(nbs_similarity)) + user_mean)\n",
    "    \n",
    "    data = score.reshape(len(score), 1)\n",
    "    index = neighbours_df.index\n",
    "    return pd.DataFrame(data = data , columns = [f'pred_user_{user_id}'], index=index)\n",
    "\n",
    "predictions_df = predict(user_id, neighbours_df, similarity_df, data)\n",
    "\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then recommend the item with the highest score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend the user_2 to read the joke_2, which has a prediction score of 14.04 !\n"
     ]
    }
   ],
   "source": [
    "best_joke = predictions_df[f'pred_user_{user_id}'].idxmax()\n",
    "best_score = predictions_df.loc[best_joke, f'pred_user_{user_id}']\n",
    "\n",
    "print(f'We recommend the user_{user_id} to read the {best_joke}, ' +\n",
    "      f'which has a prediction score of {round(best_score, 2)} !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "    The prediction of the joke 2 is higher than 10 here (which is supposed to be the maximum rating value). This is because there is no bound constraint in our problem, and also because of the small amount of data used in our example (only 10 users for 10 jokes, with a rating mean for the user 2).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Item-item filtering [[1]](#doc_01)\n",
    "\n",
    "### Principle:\n",
    "Item-item collaborative filtering is similar to user-user filtering: Rather than using similarities between users’ rating behavior to predict preferences, **item–item CF uses similarities between the rating patterns of items**. If two items tend to have the same users like and dislike them, then they are similar and users are expected to have similar preferences for similar items.\n",
    "\n",
    "<div class='alert alert-warning'>\n",
    "    We will not implement this method because it would be repetitive with the user-user filtering method.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Model based approach - Matrix Factorization (SVD)\n",
    "\n",
    "In the **model-based approach**, Collaborative Filtering models are developed **using machine learning algorithms** to predict user’s rating of unrated items.\n",
    "\n",
    "<img src='pics/matrix_factorization.png' width=800 />\n",
    "\n",
    "The idea behind **Matrix Factorization** (MF) is that **attitudes or preferences** of a user can be determined by a small number of **hidden factors**. We can call these factors as Embeddings.\n",
    "\n",
    "<div class='alert alert-warning'><b>Matrix decomposition</b> can be reformulated as <b>an optimization problem</b> with loss functions and constraints. Now the constraints are chosen based on property of our model. For example, for <i>\"Non negative Matrix Decomposition\"</i> (NMF), we want non negative elements in resultant matrices.</div>\n",
    "\n",
    "\n",
    "## 2.1 - SVD algorithm principle  [[1]](#doc_01) [[5]](#doc_05) [[6]](#doc_06)\n",
    "\n",
    "SVD algorithm is one kind of Matrix Factorization method. Let's see how it works !\n",
    "\n",
    "### 2.1.0 - Defining the Singular Value Decomposition (SVD)\n",
    "\n",
    "For a matrix $M$, its SVD factorization is the factorization of $M$ into three constituent matrices such that:\n",
    "\n",
    "$$M = U\\Sigma T^T$$ with:\n",
    "\n",
    "- $\\Sigma$ : A diagonal matrix whose values $\\sigma_i$ are the **singular values** of the decomposition\n",
    "- $U$ and $T$ are orthogonal\n",
    "\n",
    "### 2.1.1 - SVD in collaborative filtering\n",
    "\n",
    "If $M$ is the ratings matrix, $\\Sigma T^T$ transforms vectors from item-space into the intermediate vector space.\n",
    "\n",
    "- $U$ can be interpreted as the **user matrix** (size $|U|$ x $f$) which is composed of the **users preferences** coded in an intermediate vector space of dimension $f$ $\\leq \\text{min}(|U|, |I|)$.\n",
    "- $T$ can be interpreted as the **item matrix** (size $|I|$ x $f$) which is composed of the **items features** coded in an intermediate vector space of dimension $f$ $\\leq \\text{min}(|U|, |I|)$.\n",
    "\n",
    "So we have:\n",
    "\n",
    "$$\\hat{R} \\approx U\\Sigma T^T$$\n",
    "\n",
    "<div><img src='pics/svd.png' width=500/></div>\n",
    "\n",
    "Then, you can get **a user $u$'s ratings predictions** for all of the items with:\n",
    "\n",
    "$$\\hat{r}_{u,:} = u_{u, :}\\Sigma T^T$$\n",
    "\n",
    "And **an item $i$'s ratings predictions** for all of the users with:\n",
    "\n",
    "$$\\hat{r}_{:,i} = U \\Sigma t_{i, :}^T$$\n",
    "\n",
    "### 2.1.2 - Computing the SVD algorithm\n",
    "\n",
    "#### 2.1.2.1 - Rating prediction matrix\n",
    "\n",
    "The $R$ matrix is a **sparse matrix** (each user only rated a few items). There are a lot of missing rating values, so we will try to **approximate the rating matrix singular value decomposition** with a **gradient descent algorithm**.  \n",
    "\n",
    "The $\\Sigma$ matrix is a diagonal matrix, so it has only a role of scaling in the SVD factorization. To simplify, we can compute our **rating prediction matrix**  $\\hat{R}$ such that:\n",
    "\n",
    "$$\\hat{r}_{u,i}=q_i^T p_u$$ with:\n",
    "\n",
    "- $q_i$ : The $i^{th}$ **item feature vector** ($f$ x $1$ vector)\n",
    "- $p_u$ : The $u^{th}$ **user feature vector** ($f$ x $1$ vector)\n",
    "\n",
    "\n",
    "#### 2.1.2.2 - Loss function\n",
    "\n",
    "Then on a training inference we can estimate the error with the **loss function**:\n",
    "\n",
    "$$loss=\\sum_{r_{u,i}\\in R_{train}} (r_{u,i} - \\hat{r}_{u,i})^2 + \\lambda (\\lVert q_i \\lVert^2 + \\lVert p_u \\lVert^2)$$ with:\n",
    "\n",
    "- $r_{u, i}$ : **True rating** of the user $u$ on the item $i$\n",
    "- $\\hat{r}_{u, i}$ : **Predicted rating** of the user $u$ on the item $i$\n",
    "- $\\lambda$ : The **regularization** term\n",
    "\n",
    "#### 2.1.2.3 - Gradient descent\n",
    "\n",
    "The minimization is performed by a very straightforward stochastic gradient descent:\n",
    "\n",
    "$$ p_u \\leftarrow p_u + \\gamma ((r_{u,i} - \\hat{r}_{u,i}) \\cdot q_i - \\lambda p_u)$$\n",
    "$$ q_i \\leftarrow q_i + \\gamma ((r_{u,i} - \\hat{r}_{u,i}) \\cdot p_u - \\lambda q_i)$$ with:\n",
    "\n",
    "- $\\gamma$ : The **learning rate**\n",
    "\n",
    "<div class='alert alert-warning'><b>Note:</b> As you may have noticed, the <i>\"SVD algorithm\"</i> does not consist in a true Singular Value Decomposition, it is a matrix factorization algorithm inspired by the singular value decomposition of the rating matrix.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Numpy implementation of the SVD algorithm\n",
    "\n",
    "### 2.2.0 - The MovieLens dataset\n",
    "\n",
    "We will be using the **MovieLens** (small) Dataset. <a href='https://grouplens.org/datasets/movielens/latest/'>(Link here)</a>\n",
    "\n",
    "- It consists in **100,000 ratings** and 3,600 tag applications applied to **9,000 movies** by **600 users**.\n",
    "- Each rating lies **between 0.5 and 5.0**.\n",
    "\n",
    "### 2.2.1 - Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating\n",
       "0       1       1     4.0\n",
       "1       1       3     4.0\n",
       "2       1       6     4.0\n",
       "3       1      47     5.0\n",
       "4       1      50     5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_dir_path = os.path.abspath('data/movielens_dataset')\n",
    "\n",
    "movies_df = pd.read_csv(os.path.join(dataset_dir_path, 'movies.csv'), index_col=0)\n",
    "ratings_df = pd.read_csv(os.path.join(dataset_dir_path, 'ratings.csv'))\n",
    "\n",
    "\n",
    "ratings_dict = {'userID': list(ratings_df.userId),\n",
    "                'itemID': list(ratings_df.movieId),\n",
    "                'rating': list(ratings_df.rating)}\n",
    "\n",
    "ratings_df = pd.DataFrame(ratings_dict)\n",
    "\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 - Implementation of the algorithm\n",
    "\n",
    "Here is a home-made implementation of the SVD algorithm in Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SVD_numpy:\n",
    "    \"\"\"\n",
    "    Numpy implementation of the SVD algorithm for CF\n",
    "    (Author: Yannick PRUDENT)\n",
    "    \"\"\"\n",
    "    def __init__(self, ratings_df, f=100):\n",
    "        \"\"\"\n",
    "        Init of the SVD_numpy class\n",
    "        -> Stores the ratings into dataframes and matrixes\n",
    "        \"\"\"\n",
    "        # Store ratings\n",
    "        self.ratings_df = ratings_df\n",
    "        \n",
    "        self.userList = sorted(self.ratings_df['userID'].unique())\n",
    "        self.itemList = sorted(self.ratings_df['itemID'].unique())\n",
    "        n_u, n_i =  len(self.userList), len(self.itemList)\n",
    "        \n",
    "        self.item2indice = {i:self.itemList.index(int(i)) for i in self.itemList}\n",
    "        \n",
    "        self.n_samples = self.ratings_df.shape[0]\n",
    "        \n",
    "        # User x Item dataframe\n",
    "        self.ratings_ui_df = self.ratings_df.groupby(['userID', 'itemID'])['rating'].mean().unstack('itemID')\n",
    "\n",
    "        # Initialize the matrixes\n",
    "        self.U = np.random.randn(n_u, f)  # User matrix\n",
    "        self.V = np.random.randn(n_i, f)  # Item matrix\n",
    "\n",
    "\n",
    "    def generate_batches(self, entries, batch_size, shuffle=True):\n",
    "        \"\"\"\n",
    "        Generator of batches according to a batch_size\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            random.shuffle(entries)\n",
    "        for i in range(0, len(entries), batch_size):\n",
    "            yield entries[i:i+batch_size]\n",
    "\n",
    "    \n",
    "    def train(self, n_epochs=20, batch_size=16, lr=0.005, lambda_=0.02):\n",
    "        \"\"\"\n",
    "        Train the SVD algorithm over a few epochs and return the losses\n",
    "        \"\"\"\n",
    "        n_steps = math.ceil(self.n_samples/batch_size)\n",
    "        losses = np.array([])\n",
    "        # Epochs\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            loss, rmse, iter_ = 0, 0, 0\n",
    "            batches = self.generate_batches(self.ratings_df.index.to_numpy(), batch_size)\n",
    "\n",
    "            with tqdm(total=n_steps - 1, desc=f'Epoch ' + \"%02d\" % epoch) as pbar:\n",
    "                # Iterate on batches\n",
    "                for step, batch_idx in enumerate(batches):\n",
    "                    # Zero the gradients\n",
    "                    delta_U = np.zeros(self.U.shape)\n",
    "                    delta_V = np.zeros(self.V.shape)\n",
    "\n",
    "                    for idx in batch_idx:\n",
    "                        u, i, r = self.ratings_df.loc[idx]\n",
    "                        u = self.userList.index(int(u))\n",
    "                        i = self.item2indice[int(i)]  # A lot faster than self.userItem.index(int(u))\n",
    "\n",
    "                        err = r - np.dot(self.U[u,:], self.V[i,:])\n",
    "\n",
    "                        # Compute the gradients\n",
    "                        delta_U[u, :] = lr * (err * self.V[i, :] - lambda_ * self.U[u, :])\n",
    "                        delta_V[i, :] = lr * (err * self.U[u, :] - lambda_ * self.V[i, :])\n",
    "\n",
    "                        loss += err ** 2 + lambda_ * (np.linalg.norm(self.V[i, :])**2\n",
    "                                                    + np.linalg.norm(self.U[u, :])**2)\n",
    "                        rmse += err ** 2\n",
    "                        \n",
    "                        iter_ += 1\n",
    "\n",
    "                    # Descent gradient step\n",
    "                    self.U += delta_U\n",
    "                    self.V += delta_V\n",
    "\n",
    "                    # Update the progress bar infos\n",
    "                    if (step%(n_steps//100) == 0) or (step == n_steps - 1):\n",
    "                        loss_bar = loss.item()/(iter_ + 1)\n",
    "                        rmse_bar = np.sqrt(rmse/(iter_ + 1))\n",
    "                        pbar.set_postfix({'Loss':loss_bar, 'RMSE':rmse_bar})\n",
    "                        pbar.update(n=min(n_steps//100, n_steps - 1 - step))\n",
    "            \n",
    "            loss /= self.ratings_df.shape[0]\n",
    "            losses = np.append(losses, loss.item())\n",
    "        return losses\n",
    "    \n",
    "    \n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        Predict the rating of an item for a user\n",
    "        \"\"\"\n",
    "        u = self.userList.index(int(user_id))\n",
    "        i = self.itemList.index(int(item_id))\n",
    "        pred = np.dot(self.U[u, :], self.V[i, :])\n",
    "        return pred\n",
    "\n",
    "    \n",
    "    def predict_unrated_items(self, user_id):\n",
    "        \"\"\"\n",
    "        Select the unrated items of a user and return their predictions\n",
    "        \"\"\"\n",
    "        user_ratings = self.ratings_ui_df.loc[user_id]\n",
    "        unrated_items = np.where(user_ratings.isnull())[0]\n",
    "        \n",
    "        u = self.userList.index(int(user_id))\n",
    "        # Prediction\n",
    "        ratings_user = np.dot(self.U[u, :], self.V[unrated_items, :].T)\n",
    "        \n",
    "        unrated_items_labels = user_ratings.index[np.where(user_ratings.isnull())[0]]\n",
    "\n",
    "        ratings_user_df = pd.DataFrame(ratings_user, columns=[f'user_{user_id}'], \n",
    "                                       index=unrated_items_labels)\n",
    "        return ratings_user_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 - Training on the MovieLens dataset\n",
    "\n",
    "Let's train our algorithm on 20 epochs with batches of size 5. \n",
    "\n",
    "(The training may take up to 8-10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 20167/20167 [00:24<00:00, 835.23it/s, Loss=17.6, RMSE=4.14]\n",
      "Epoch 02: 100%|██████████| 20167/20167 [00:24<00:00, 810.13it/s, Loss=6.62, RMSE=2.49]\n",
      "Epoch 03: 100%|██████████| 20167/20167 [00:25<00:00, 800.32it/s, Loss=3.81, RMSE=1.84]\n",
      "Epoch 04: 100%|██████████| 20167/20167 [00:25<00:00, 800.82it/s, Loss=2.82, RMSE=1.55]\n",
      "Epoch 05: 100%|██████████| 20167/20167 [00:25<00:00, 784.61it/s, Loss=2.3, RMSE=1.38] \n",
      "Epoch 06: 100%|██████████| 20167/20167 [00:26<00:00, 771.38it/s, Loss=1.99, RMSE=1.26]\n",
      "Epoch 07: 100%|██████████| 20167/20167 [00:24<00:00, 815.81it/s, Loss=1.77, RMSE=1.17]\n",
      "Epoch 08: 100%|██████████| 20167/20167 [00:23<00:00, 844.07it/s, Loss=1.62, RMSE=1.11]\n",
      "Epoch 09: 100%|██████████| 20167/20167 [00:23<00:00, 841.05it/s, Loss=1.49, RMSE=1.05]\n",
      "Epoch 10: 100%|██████████| 20167/20167 [00:24<00:00, 807.51it/s, Loss=1.4, RMSE=1]     \n",
      "Epoch 11: 100%|██████████| 20167/20167 [00:25<00:00, 802.09it/s, Loss=1.33, RMSE=0.969]\n",
      "Epoch 12: 100%|██████████| 20167/20167 [00:24<00:00, 817.47it/s, Loss=1.27, RMSE=0.938]\n",
      "Epoch 13: 100%|██████████| 20167/20167 [00:25<00:00, 777.26it/s, Loss=1.21, RMSE=0.911]\n",
      "Epoch 14: 100%|██████████| 20167/20167 [00:24<00:00, 823.87it/s, Loss=1.17, RMSE=0.888]\n",
      "Epoch 15: 100%|██████████| 20167/20167 [00:24<00:00, 839.50it/s, Loss=1.13, RMSE=0.868]\n",
      "Epoch 16: 100%|██████████| 20167/20167 [00:24<00:00, 829.95it/s, Loss=1.1, RMSE=0.85]  \n",
      "Epoch 17: 100%|██████████| 20167/20167 [00:24<00:00, 832.85it/s, Loss=1.07, RMSE=0.834]\n",
      "Epoch 18: 100%|██████████| 20167/20167 [00:24<00:00, 838.98it/s, Loss=1.05, RMSE=0.82] \n",
      "Epoch 19: 100%|██████████| 20167/20167 [00:24<00:00, 822.63it/s, Loss=1.02, RMSE=0.807]\n",
      "Epoch 20: 100%|██████████| 20167/20167 [29:08<00:00, 11.53it/s, Loss=1.01, RMSE=0.797]  \n"
     ]
    }
   ],
   "source": [
    "# Seed for reproductibility\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "# Run training on 20 epochs\n",
    "svd = SVD_numpy(ratings_df, f=20)\n",
    "losses = svd.train(n_epochs=20, batch_size=5, lr=0.01, lambda_=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 - Predictions\n",
    "\n",
    "Now that the SVD model has been trained, we want to recommend a movie for the user_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of user_1's favorite movies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itemID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5060</th>\n",
       "      <td>M*A*S*H (a.k.a. MASH) (1970)</td>\n",
       "      <td>Comedy|Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>Young Frankenstein (1974)</td>\n",
       "      <td>Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>L.A. Confidential (1997)</td>\n",
       "      <td>Crime|Film-Noir|Mystery|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>Conan the Barbarian (1982)</td>\n",
       "      <td>Action|Adventure|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>Face/Off (1997)</td>\n",
       "      <td>Action|Crime|Drama|Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title                            genres\n",
       "itemID                                                                \n",
       "5060    M*A*S*H (a.k.a. MASH) (1970)                  Comedy|Drama|War\n",
       "1278       Young Frankenstein (1974)                    Comedy|Fantasy\n",
       "1617        L.A. Confidential (1997)  Crime|Film-Noir|Mystery|Thriller\n",
       "1587      Conan the Barbarian (1982)          Action|Adventure|Fantasy\n",
       "1573                 Face/Off (1997)       Action|Crime|Drama|Thriller"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend the user_1 to see \"Hamlet (1964)\", which is a \"drama\" movie !\n"
     ]
    }
   ],
   "source": [
    "user_id = 1\n",
    "\n",
    "# User's 5 favorite movies\n",
    "print(f'Some of user_{user_id}\\'s favorite movies:')\n",
    "ratings_user = svd.ratings_ui_df.loc[user_id].sort_values(ascending=False)\n",
    "fav_movies = movies_df.loc[ratings_user.iloc[:5].index]\n",
    "display(fav_movies)\n",
    "\n",
    "# Recommendation: Highest prediction\n",
    "predict_user = svd.predict_unrated_items(user_id)\n",
    "best_movie_idx = predict_user.iloc[:, 0].idxmax()\n",
    "best_movie = movies_df.loc[best_movie_idx]\n",
    "\n",
    "print(f'We recommend the user_{user_id} to see \"{best_movie.title}\", ' +\n",
    "      f'which is a \"{best_movie.genres.lower()}\" movie !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'><b>Note:</b> In order to avoid reduduncy, instead of just recommending the movie with the highest predicted rating, we could also randomly choose a movie among the k-highest predicted ratings movies (with k a positive integer).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Scikit-surprise library [[4]](#doc_04) [[5]](#doc_05)\n",
    "\n",
    "<img src='pics/logo_surprise.svg' width=200 />\n",
    "\n",
    "The **scikit-surprise** package has been specially **developed to make recommendation based on Collaborative Filtering easy**. It has default implementation for a variety of CF algorithms. **[See the documentation here](https://surprise.readthedocs.io/en/v1.1.1/)**.\n",
    "\n",
    "## 3.0 - Installation\n",
    "Here is how you can install the library either with **pip** or **conda**:\n",
    "\n",
    "#### Pip installation:\n",
    "\n",
    "```bash\n",
    "pip install scikit-surprise\n",
    "```\n",
    "\n",
    "#### Conda installation:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge scikit-surprise\n",
    "```\n",
    "\n",
    "Now let's see how to quickly get started with this library !\n",
    "\n",
    "## 3.1 - Load dataset in scikit-surprise\n",
    "\n",
    "We will re-use the **MovieLens dataset** that we already used in the part **[(2.2)](#2.2.0---The-MovieLens-dataset)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset\n",
    "\n",
    "# Reader of the dataset\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(ratings_df[['userID', 'itemID', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - A few algorithms for Collaborative Filtering\n",
    "\n",
    "### 3.2.1 - KNN (K-Nearest Neighbours)\n",
    "\n",
    "This is a **memory based model** (you can choose **either user-user or item-item**). We explained this algorithm in the part **[(1)](#1---Memory-based-approach)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9740  0.9788  0.9579  0.9774  0.9855  0.9747  0.0092  \n",
      "MAE (testset)     0.7468  0.7535  0.7423  0.7513  0.7572  0.7502  0.0052  \n",
      "Fit time          0.45    0.46    0.47    0.47    0.47    0.47    0.01    \n",
      "Test time         1.19    1.14    1.15    1.11    1.11    1.14    0.03    \n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNBasic\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# knn\n",
    "algo = KNNBasic(k=40, \n",
    "                min_k=1, \n",
    "                sim_options={'name': 'pearson_baseline', 'user_based': True}, # user_based = True -> user-user\n",
    "                verbose=False)\n",
    "\n",
    "results = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - SVD (Singular Value Decomposition)\n",
    "\n",
    "We explained this algorithm in the part **[(2.1)](#2.1.0---Defining-the-Singular-Value-Decomposition)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9462  0.9467  0.9463  0.9382  0.9463  0.9448  0.0033  \n",
      "MAE (testset)     0.7328  0.7278  0.7319  0.7256  0.7293  0.7295  0.0026  \n",
      "Fit time          6.26    6.34    6.20    6.28    6.28    6.27    0.05    \n",
      "Test time         0.16    0.09    0.15    0.09    0.09    0.11    0.03    \n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "# svd\n",
    "algo = SVD(n_factors=100,\n",
    "           n_epochs=30,\n",
    "           biased=False,\n",
    "           lr_all = 0.05,\n",
    "           reg_all=0.02)\n",
    "\n",
    "results = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 - NMF (Non-negative Matrix Factorization)\n",
    "\n",
    "This algorithm is **very similar to SVD**. The prediction $\\hat{r}_{u,i}$ is set as $\\hat{r}_{u,i}=q_i^T p_u$, where **user and item factors are kept positive**. \n",
    "\n",
    "[(See the NMF surprise documentation for more details)](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm NMF on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9196  0.9221  0.9261  0.9175  0.9274  0.9225  0.0038  \n",
      "MAE (testset)     0.7048  0.7052  0.7068  0.7023  0.7100  0.7058  0.0025  \n",
      "Fit time          5.96    5.95    6.04    6.54    5.87    6.07    0.24    \n",
      "Test time         0.16    0.10    0.10    0.23    0.10    0.14    0.05    \n"
     ]
    }
   ],
   "source": [
    "from surprise import NMF\n",
    "\n",
    "# nmf\n",
    "algo = NMF(n_factors=15,\n",
    "           n_epochs=50,\n",
    "           biased=False)\n",
    "\n",
    "results = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We saw in part [(1)](#1---Memory-based-approach) and in part [(2)](#2---Model-based-approach---Matrix-Factorization-(SVD)\n",
    ") some examples of the most common algorithms used in Collaborative Filtering. \n",
    "\n",
    "- **Memory-Based methods** are some **easy-to-use approaches** as no training or optimization is involved. But their performance decrease when we have sparse data which hinders scalability of these approaches for most of the real-world problems.\n",
    "\n",
    "\n",
    "- **Model-Based methods** are widely used and can lead to **really good predictions**: In 2006 Netflix hosted a million dollar competition for the best movie recommendation algorithm and **most of the leading entries used SVD** (Matrix Factorization CF algorithm). \n",
    "\n",
    "\n",
    "- We ended this notebook [(3)](#3.0---Installation) with a quick tutorial on the widely used CF python library **scikit-surprise**. It is an easy way to compute most of the existing collaborative filtering algorithms. Moreover, the trainings of the model-based methods are **really fast**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
